{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# re-Search\n",
    "\n",
    "### Phase 2\n",
    "Project by Naseer Ansari and Vijay Rajasekar\n",
    "\n",
    "60-538: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Recap of what we implemented in Phase 1\n",
    "\n",
    "* StandardAnalyzer indexing and searching using Whoosh  \n",
    "\n",
    "* Simple frontend using Vue.js  \n",
    "\n",
    "* REST API using Flask to serve search results and files to frontend\n",
    "\n",
    "### Challenges faced\n",
    "\n",
    "* Larged size indexes affecting search speed\n",
    "\n",
    "* Dataset too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 1: Solving the challenges\n",
    "\n",
    "## Choosing a new dataset\n",
    "\n",
    "* Dataset large enough to provide good search results but also not too large to affecrt performance while running NLP tasks\n",
    "\n",
    "* AMiner Citation Network V1 - 629,814 papers and >632,752 citation relationships\n",
    "\n",
    "* Perfect size for running NLP tasks and providing good search results, but still pretty large for running NetworkX PageRank\n",
    "\n",
    "## Index size\n",
    "\n",
    "* Implemented segmented indexing, dividing large index size into segments\n",
    "\n",
    "* Reduces memory overhead while performing searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Dr. Lu's to-do list for the search engine\n",
    "\n",
    "1. Queries longer than one word\n",
    "\n",
    "2. Query expansion eg. \"deep learning\" -> \"deep neural network\"\n",
    "\n",
    "3. Phrases or keywords or concepts in computer science\n",
    "\n",
    "4. Queries occuring in title should have a higher weight than in abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Queries longer than one word\n",
    "\n",
    "* Using Whoosh's Phrase Query class to implement Phrase Search\n",
    "\n",
    "`class whoosh.query.Phrase(fieldname, words, slop=1, boost=1.0, char_ranges=None)`\n",
    "* slop – the number of words allowed between each “word” in the phrase; the default of 1 means the phrase must match exactly.\n",
    "\n",
    "* boost – a boost factor that is applied to the raw score of documents matched by this query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Popular phrases and queries in Computer Science\n",
    "\n",
    "* Built a web crawler/parser to crawl through the links and obtain terms from each of these categories\n",
    "\n",
    "    <img src=\"./webopedia.png\" title=\"webopedia\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Popular phrases and queries in Computer Science\n",
    "\n",
    "* Determined occurence of each of these terms in word embeddding training dataset\n",
    "\n",
    "* Concatenated each occurence into a single word and proceeded to determine embedding\n",
    "\n",
    "* Used these embeddings for query expansion as well, thereby achieving the second task in the todo list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Count of word occurence in training dataset\n",
    "\n",
    "<img src=\"./wordoccurence.png\" title=\"wordoccurence\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Providing higher priority to Title field - BM25F Algorithm \n",
    "\n",
    "* BM25 algorithm\n",
    "    * Innovation of traditional TF-IDF \n",
    "    \n",
    "    * Faster dampening of term frequency in BM25 \n",
    "     \n",
    "    * Document with just a single word repeated 1000 times will have lower score in BM25 than TF-IDF\n",
    "    \n",
    "    * Takes document length into consideration for calculating document relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://opensourceconnections.com/blog/uploads/2015/TF1.png\" title=\"BM25 vs traditional TF\" />\n",
    "\n",
    "https://opensourceconnections.com/blog/2016/10/19/bm25f-in-lucene/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "* BM25F \n",
    "    * Improvement of BM25, designed for Structured Information Retrieval i.e documents divided into fields such as title and abstract\n",
    "    \n",
    "    * Applies BM25 algorithm across multiple fields\n",
    "    \n",
    "    * Whoosh supports field boosting (providing higher weight to a certain field)\n",
    "    \n",
    "`schema = Schema(title=TEXT(field_boost=2.0), body=TEXT)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3: Search Engine Requirements\n",
    "\n",
    "* PageRank\n",
    "\n",
    "* Word Embedding\n",
    "\n",
    "* Classification\n",
    "\n",
    "* Similar paper retrieval and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PageRank\n",
    "\n",
    "* Dataset still too large to perform NetworkX PageRank with good performance\n",
    "\n",
    "* graph-tool library (https://graph-tool.skewed.de/)\n",
    "\n",
    "    * C graph library with Python bindings\n",
    "    \n",
    "    * Built on top of Boost Graph Library\n",
    "    \n",
    "    * Very nice integration with Linux packages\n",
    "    \n",
    "    * Faster than NetworkX\n",
    "    \n",
    "<img src=\"./gtvsnx.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word Embedding\n",
    "\n",
    "* FastText\n",
    "    * Word embedding technique introudced by Facebook AI Research lab (FAIR) in 2016\n",
    "    \n",
    "    * Improvement of word2vec\n",
    "    \n",
    "    * Each word is represented as a bag of character n-grams\n",
    "    \n",
    "    * Vector representation is associated to each character n-gram; words being represented as the sum of these representations\n",
    "    \n",
    "    * Useful for obtaining embeddings from research papers which contain rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classification\n",
    "\n",
    "* Performed using Fasttext\n",
    "\n",
    "* Used SIGMOD, ICSE and VLDB titles as training data\n",
    "\n",
    "* Parsed data using Pandas, preprocessed to achieve below format required for fasttext classification\n",
    "\n",
    "`__label__vldb a new service for customer care based on the trentorise bigdata platform`\n",
    "\n",
    "* Classification Algorithm: Bag of Tricks (by Facebook AI Research Team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Similar Paper Retrieval and Clustering\n",
    "\n",
    "* Applied Doc2Vec on full dataset, reduced document dimensionality using PCA and applied KMeans Clustering\n",
    "\n",
    "* Similar document retrieval on frontend performed using a REST endpoint \n",
    "<img src=\"./kmeans.png\" title=\"kmeans\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Repo links\n",
    "\n",
    "https://github.com/vijayRT/re-Search (backend Python)\n",
    "\n",
    "https://github.com/vijayRT/re-SearchVue (frontend JS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks\n",
    "\n",
    "This presentation was brought to you with the help of Jupyter Notebook\n",
    "\n",
    "# Any questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "* https://whoosh.readthedocs.io/en/latest/index.html\n",
    "\n",
    "* https://opensourceconnections.com/blog/2016/10/19/bm25f-in-lucene/\n",
    "\n",
    "* Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. ArnetMiner: Extraction and Mining of Academic Social Networks. In Proceedings of the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD'2008). pp.990-998. [PDF] [Slides] [System] [API]\n",
    "\n",
    "* P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information\n",
    "\n",
    "* A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
